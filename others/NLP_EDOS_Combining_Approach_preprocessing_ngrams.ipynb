{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfbfNBGDkuya"
      },
      "source": [
        "# Combining the best tested approaches\n",
        "\n",
        "*   WordPiece Tokenization\n",
        "*   Hyperparameter tuning\n",
        "*   Larger Electra Model\n",
        "*   Data Augmentation\n",
        "*   Cross-Validation\n",
        "\n",
        "\n",
        "Not Included (could be used)\n",
        "*   Word Embeddings\n",
        "*   Advanced or custom Models\n",
        "*   Advanced algorithms for Hyperparameter optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWbJj4u6lbbz"
      },
      "source": [
        "Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXAoAwM3leoD",
        "outputId": "57f34ebd-a9a2-4437-ca0b-fdc5ac3cf120"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers[torch] datasets accelerate evaluate tensorboard scikit-learn nlpaug alive-progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVAE6_lSlx1H",
        "outputId": "ba128c5e-2c35-4458-e90e-22d9468b6794"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.device('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0j_rYpilxPJ",
        "outputId": "015fc86f-5675-4b73-b97f-7fdd60ae5119"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0koKKqcblNng"
      },
      "source": [
        "Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmg-csrNkf1a",
        "outputId": "82b992ce-379b-4c71-a3ec-c5b5e3857222"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/nlp/edos_labelled_aggregated.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Filter the relevant columns\n",
        "df = df[['text', 'label_sexist', 'split', 'label_category']]\n",
        "\n",
        "# Encode the labels for the first task\n",
        "df['label_sexist'] = df['label_sexist'].apply(lambda x: 1 if x == 'sexist' else 0)\n",
        "\n",
        "# Filter sexist sentences\n",
        "sexist_df = df[df['label_sexist'] == 1]\n",
        "\n",
        "classes_labels = sexist_df['label_category'].unique().copy()\n",
        "\n",
        "print(classes_labels)\n",
        "\n",
        "# Split the data\n",
        "train_sexist_df = sexist_df[sexist_df['split'] == 'train']\n",
        "val_sexist_df = sexist_df[sexist_df['split'] == 'dev']\n",
        "\n",
        "train_sexist_texts = train_sexist_df['text'].tolist()\n",
        "train_sexist_labels = train_sexist_df['label_category'].tolist()\n",
        "val_sexist_texts = val_sexist_df['text'].tolist()\n",
        "val_sexist_labels = val_sexist_df['label_category'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piLr6lVzl4SN"
      },
      "source": [
        "Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRGiFna0lXo_"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode the categories\n",
        "label_encoder = LabelEncoder()\n",
        "train_sexist_labels = label_encoder.fit_transform(train_sexist_labels)\n",
        "val_sexist_labels = label_encoder.transform(val_sexist_labels)\n",
        "\n",
        "num_labels = len(label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XkCa9hEm6cr"
      },
      "source": [
        "Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFUptfw0l3Ws",
        "outputId": "3f0d2daa-9b80-4861-8fcb-d743e524b378"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# The set of stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove stopwords and apply lemmatization\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Preprocess the texts\n",
        "train_sexist_texts = [preprocess_text(text) for text in train_sexist_texts]\n",
        "val_sexist_texts = [preprocess_text(text) for text in val_sexist_texts]\n",
        "\n",
        "# Feature extraction with unigrams and bigrams\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "train_features = vectorizer.fit_transform(train_sexist_texts)\n",
        "val_features = vectorizer.transform(val_sexist_texts)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_features = torch.tensor(train_features.toarray(), dtype=torch.float32)\n",
        "val_features = torch.tensor(val_features.toarray(), dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_sexist_labels, dtype=torch.long)\n",
        "val_labels = torch.tensor(val_sexist_labels, dtype=torch.long)\n",
        "\n",
        "# Save the processed data for further use in model training\n",
        "torch.save((train_features, train_labels), 'train_data.pt')\n",
        "torch.save((val_features, val_labels), 'val_data.pt')\n",
        "\n",
        "print(\"Data preprocessing complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idChOH7bnUCC"
      },
      "source": [
        "Augmented Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNLMqrFfnWLI",
        "outputId": "e4878339-0a0f-495e-d435-b9e03409db74"
      },
      "outputs": [],
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "combined_train_dataframe = pd.DataFrame({'text': train_sexist_texts, 'label': train_sexist_labels})\n",
        "\n",
        "\n",
        "# Define a synonym augmentation function\n",
        "def augment_text(text, aug_max):\n",
        "    aug = naw.SynonymAug(aug_src='wordnet', aug_max=aug_max)\n",
        "    augmented_text = aug.augment(text)\n",
        "    return augmented_text\n",
        "\n",
        "\n",
        "\n",
        "for idx, row in combined_train_dataframe.iterrows():\n",
        "  # create an augmented_text\n",
        "  augmented_tx_1 = augment_text(row['text'], aug_max=1)[0]\n",
        "  augmented_tx_2 = augment_text(row['text'], aug_max=2)[0]\n",
        "  augmented_tx_3 = augment_text(row['text'], aug_max=3)[0]\n",
        "\n",
        "  # append to the next row dataframe the augmented_text and the label at that row\n",
        "  combined_train_dataframe.loc[idx + 0.1] = [augmented_tx_1, row['label']]\n",
        "  combined_train_dataframe.loc[idx + 0.2] = [augmented_tx_2, row['label']]\n",
        "  combined_train_dataframe.loc[idx + 0.3] = [augmented_tx_3, row['label']]\n",
        "\n",
        "# Sort the dataframe by index to reposition the inserted rows\n",
        "combined_train_dataframe = combined_train_dataframe.sort_index().reset_index(drop=True)\n",
        "\n",
        "\n",
        "combined_train_texts = combined_train_dataframe['text'].tolist()\n",
        "combined_train_labels = combined_train_dataframe['label'].tolist()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UpHu-8Hnjds"
      },
      "source": [
        "Preparation to Cross Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sj9uq-HngUB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Convert texts and labels to numpy arrays for KFold\n",
        "combined_train_texts_np = np.array(combined_train_texts)\n",
        "combined_train_labels_np = np.array(combined_train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBb7bzwhmT3_"
      },
      "outputs": [],
      "source": [
        "# Create a custom dataset class\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaIu9zCmmdYq",
        "outputId": "8c6c4bff-083a-41a3-ab2f-07c893028e5b"
      },
      "outputs": [],
      "source": [
        "from transformers import ElectraForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=200,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        ")\n",
        "\n",
        "# Initialize the model\n",
        "model = ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels=num_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg4oJqQCoMDY"
      },
      "source": [
        "Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw2TVoI9mmEu"
      },
      "outputs": [],
      "source": [
        "# Initialize the BERT tokenizer (WordPiece)\n",
        "\n",
        "from transformers import BertTokenizer, ElectraTokenizer\n",
        "# tokenizer_wordpiece = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer_wordpiece = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
        "\n",
        "def tokenizer(texts):\n",
        "    return tokenizer_wordpiece(texts, padding=True, truncation=True, max_length=128, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hxmy0K2MoTfS"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RVcR2rJzoRQF",
        "outputId": "41115f88-0932-489c-efaf-cb17933ddbb2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cross-validation loop\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "fold = 0\n",
        "for train_index, val_index in kf.split(combined_train_texts_np):\n",
        "    fold += 1\n",
        "    print(f\"Training fold {fold}...\")\n",
        "\n",
        "    # Split the data\n",
        "    train_texts_fold = combined_train_texts_np[train_index].tolist()\n",
        "    val_texts_fold = combined_train_texts_np[val_index].tolist()\n",
        "    train_labels_fold = combined_train_labels_np[train_index].tolist()\n",
        "    val_labels_fold = combined_train_labels_np[val_index].tolist()\n",
        "\n",
        "    # Tokenize the data\n",
        "    train_encodings_fold = tokenizer(train_texts_fold)\n",
        "    val_encodings_fold = tokenizer(val_texts_fold)\n",
        "\n",
        "    # Create dataset\n",
        "    train_dataset_fold = TextDataset(train_encodings_fold, train_labels_fold)\n",
        "    val_dataset_fold = TextDataset(val_encodings_fold, val_labels_fold)\n",
        "\n",
        "    # Initialize the trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset_fold,\n",
        "        eval_dataset=val_dataset_fold,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    # Predictions\n",
        "    predictions = trainer.predict(val_dataset_fold)\n",
        "    predicted_labels = torch.tensor(predictions.predictions).argmax(dim=-1)\n",
        "\n",
        "    # F1 Score and Confusion Matrix\n",
        "    precision = precision_score(val_labels_fold, predicted_labels, average='weighted')\n",
        "    recall = recall_score(val_labels_fold, predicted_labels, average='weighted')\n",
        "    f1 = f1_score(val_labels_fold, predicted_labels, average='weighted')\n",
        "    conf_matrix = confusion_matrix(val_labels_fold, predicted_labels)\n",
        "\n",
        "    print(f\"Fold {fold} F1 Score: {f1}\")\n",
        "\n",
        "    # Visualize the confusion matrix\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=label_encoder.classes_)\n",
        "    fig, ax = plt.subplots()\n",
        "    disp.plot(ax=ax)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "id": "zsrpiouu6yN-",
        "outputId": "73dd7d07-57a5-44eb-bbaf-73b38c2c2e51"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import ElectraTokenizer, ElectraForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/nlp/edos_test_category_5.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Filter the relevant columns\n",
        "df = df[['text', 'label_sexist', 'split', 'label_category']]\n",
        "\n",
        "# Encode the labels for the first task\n",
        "df['label_sexist'] = df['label_sexist'].apply(lambda x: 1 if x == 'sexist' else 0)\n",
        "\n",
        "# Filter sexist sentences\n",
        "sexist_df = df[df['label_sexist'] == 1]\n",
        "\n",
        "# Split the data\n",
        "test_df = sexist_df[sexist_df['split'] == 'test']\n",
        "test_texts = test_df['text'].tolist()\n",
        "test_labels = test_df['label_category'].tolist()\n",
        "\n",
        "# Encode the categories\n",
        "test_labels = label_encoder.fit_transform(test_labels)\n",
        "\n",
        "num_labels = len(label_encoder.classes_)\n",
        "\n",
        "# Tokenize the test texts\n",
        "test_encodings = tokenizer(test_texts)\n",
        "\n",
        "test_dataset = TextDataset(test_encodings, test_labels)\n",
        "\n",
        "# Initialize the trainer with the training arguments and model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=None,  # No need to train again\n",
        "    eval_dataset=test_dataset  # Use test dataset for evaluation\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "predicted_labels = torch.tensor(predictions.predictions).argmax(dim=-1)\n",
        "\n",
        "# F1 Score and Confusion Matrix\n",
        "precision = precision_score(test_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(test_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(test_labels, predicted_labels, average='weighted')\n",
        "conf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
        "\n",
        "print(\"F1 Score on Test Data:\", f1)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=label_encoder.classes_)\n",
        "fig, ax = plt.subplots()\n",
        "disp.plot(ax=ax)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
