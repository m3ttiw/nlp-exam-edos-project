{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vmu7vAXxfMs",
        "outputId": "1fb1002b-1d8e-4a6d-84eb-109a0189f4e5"
      },
      "outputs": [],
      "source": [
        "!pip install nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L87lnKy9KlOT",
        "outputId": "24af5b98-7590-4b43-d36b-63f899ebfc41"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "njDMe9C1Kh9A",
        "outputId": "97c604bb-4aa7-4241-a342-1ad20167feb8"
      },
      "outputs": [],
      "source": [
        "# upload the datasets\n",
        "\n",
        "from google.colab import files\n",
        "print(\"Upload edos_labelled_aggregated.csv\")\n",
        "files.upload()\n",
        "print(\"Upload edos_test_category_5.csv\")\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "229KnoXiKfgO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import time\n",
        "import nlpaug.augmenter.word as naw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8-uofrHKqVX"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "train_df = pd.read_csv('edos_labelled_aggregated.csv')\n",
        "test_df = pd.read_csv('edos_test_category_5.csv')\n",
        "\n",
        "# Filter the relevant columns\n",
        "train_df = train_df[['text', 'label_sexist', 'split', 'label_category']]\n",
        "\n",
        "# Filter sexist sentences\n",
        "sexist_df = train_df[train_df['label_sexist'] == 'sexist']\n",
        "\n",
        "# Split the data\n",
        "train_sexist_df = sexist_df[sexist_df['split'] == 'train']\n",
        "val_sexist_df = sexist_df[sexist_df['split'] == 'dev']\n",
        "\n",
        "train_sexist_texts = train_sexist_df['text'].tolist()\n",
        "train_sexist_labels = train_sexist_df['label_category'].tolist()\n",
        "val_sexist_texts = val_sexist_df['text'].tolist()\n",
        "val_sexist_labels = val_sexist_df['label_category'].tolist()\n",
        "test_texts = test_df['text'].tolist()\n",
        "test_labels = test_df['label_category'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdN8Ud-yK4B7"
      },
      "outputs": [],
      "source": [
        "# Encode the categories\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_sexist_labels = label_encoder.fit_transform(train_sexist_labels)\n",
        "val_sexist_labels = label_encoder.transform(val_sexist_labels)\n",
        "test_labels = label_encoder.transform(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dapLlQ-gLRsd",
        "outputId": "8adf0340-4ea6-4900-d4a0-d880b5e3807d"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# The set of stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove stopwords and apply lemmatization\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "\n",
        "# Preprocess the texts\n",
        "train_sexist_texts = [preprocess_text(text) for text in train_sexist_texts]\n",
        "val_sexist_texts = [preprocess_text(text) for text in val_sexist_texts]\n",
        "test_texts = [preprocess_text(text) for text in test_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNmVlgJXLjh-",
        "outputId": "8a37c0da-314f-4497-a39e-9dc0806dc48f"
      },
      "outputs": [],
      "source": [
        "# Combine train and validation sets for training the SVM model\n",
        "combined_train_texts = train_sexist_texts + val_sexist_texts\n",
        "combined_train_labels = list(train_sexist_labels) + list(val_sexist_labels)\n",
        "\n",
        "# Create a DataFrame to store the augmented texts and labels\n",
        "combined_train_dataframe = pd.DataFrame({'text': combined_train_texts, 'label': combined_train_labels})\n",
        "\n",
        "# Define a synonym augmentation function\n",
        "def augment_text(text, aug_max):\n",
        "    aug = naw.SynonymAug(aug_src='wordnet', aug_max=aug_max)\n",
        "    augmented_text = aug.augment(text)\n",
        "    return augmented_text\n",
        "\n",
        "\n",
        "\n",
        "for idx, row in combined_train_dataframe.iterrows():\n",
        "  # create an augmented_text\n",
        "  augmented_tx_1 = augment_text(row['text'], aug_max=1)[0]\n",
        "  augmented_tx_2 = augment_text(row['text'], aug_max=2)[0]\n",
        "  augmented_tx_3 = augment_text(row['text'], aug_max=3)[0]\n",
        "\n",
        "  # append to the next row dataframe the augmented_text and the label at that row\n",
        "  combined_train_dataframe.loc[idx + 0.1] = [augmented_tx_1, row['label']]\n",
        "  combined_train_dataframe.loc[idx + 0.2] = [augmented_tx_2, row['label']]\n",
        "  combined_train_dataframe.loc[idx + 0.3] = [augmented_tx_3, row['label']]\n",
        "\n",
        "# Sort the dataframe by index to reposition the inserted rows\n",
        "combined_train_dataframe = combined_train_dataframe.sort_index().reset_index(drop=True)\n",
        "\n",
        "combined_train_texts = combined_train_dataframe['text'].tolist()\n",
        "combined_train_labels = combined_train_dataframe['label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W37DfnXcLxH9",
        "outputId": "d24d928d-b01b-46f0-e690-3e73c5a3ff2b"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()  # Record the start time\n",
        "\n",
        "# Define the SVM pipeline with TF-IDF vectorizer\n",
        "svm_pipeline = make_pipeline(\n",
        "    TfidfVectorizer(),\n",
        "    SVC(kernel='linear', C=1.0)  # Linear kernel\n",
        ")\n",
        "\n",
        "# Implement k-fold cross-validation\n",
        "kf = KFold(n_splits=3)  # Define the k-fold with 5 splits\n",
        "\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_list = []\n",
        "\n",
        "for train_index, val_index in kf.split(combined_train_texts):\n",
        "    train_texts = [combined_train_texts[i] for i in train_index]\n",
        "    val_texts = [combined_train_texts[i] for i in val_index]\n",
        "    train_labels = [combined_train_labels[i] for i in train_index]\n",
        "    val_labels = [combined_train_labels[i] for i in val_index]\n",
        "\n",
        "    # Train the model\n",
        "    svm_pipeline.fit(train_texts, train_labels)\n",
        "\n",
        "    # Predict on the validation set\n",
        "    val_predicted_labels = svm_pipeline.predict(val_texts)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(val_labels, val_predicted_labels)\n",
        "    precision = precision_score(val_labels, val_predicted_labels, average='weighted')\n",
        "    recall = recall_score(val_labels, val_predicted_labels, average='weighted')\n",
        "    f1 = f1_score(val_labels, val_predicted_labels, average='weighted')\n",
        "\n",
        "    # Store the results\n",
        "    accuracy_list.append(accuracy)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1_list.append(f1)\n",
        "\n",
        "# Calculate the average scores\n",
        "average_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "average_precision = sum(precision_list) / len(precision_list)\n",
        "average_recall = sum(recall_list) / len(recall_list)\n",
        "average_f1 = sum(f1_list) / len(f1_list)\n",
        "\n",
        "print(\"Average Accuracy:\", average_accuracy)\n",
        "print(\"Average Precision:\", average_precision)\n",
        "print(\"Average Recall:\", average_recall)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "end_time = time.time()  # Record the end time\n",
        "execution_time = end_time - start_time  # Calculate the execution time\n",
        "print(f\"SVM with k-fold cross-validation execution time: {execution_time} seconds\")\n",
        "\n",
        "# # Train the final model on the entire training set\n",
        "# svm_pipeline.fit(combined_train_texts, combined_train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "1QtNK6sKLx3T",
        "outputId": "08ce778d-c88e-4922-f87b-f1a15508c391"
      },
      "outputs": [],
      "source": [
        "# Predict on the test data\n",
        "predicted_labels_svm = svm_pipeline.predict(test_texts)\n",
        "\n",
        "# Evaluate the final SVM model on the test set\n",
        "accuracy = accuracy_score(test_labels, predicted_labels_svm)\n",
        "precision = precision_score(test_labels, predicted_labels_svm, average='weighted')\n",
        "recall = recall_score(test_labels, predicted_labels_svm, average='weighted')\n",
        "f1 = f1_score(test_labels, predicted_labels_svm, average='weighted')\n",
        "conf_matrix_svm = confusion_matrix(test_labels, predicted_labels_svm)\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "print(\"Test Precision:\", precision)\n",
        "print(\"Test Recall:\", recall)\n",
        "print(\"Test F1 Score:\", f1)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_svm, display_labels=label_encoder.classes_)\n",
        "fig, ax = plt.subplots()\n",
        "disp.plot(ax=ax)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
